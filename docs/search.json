[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Business Statistics",
    "section": "",
    "text": "“Whatever you would make habitual, practice it; and if you would not make a thing habitual, do not practice it, but accustom yourself to something else.” Epictetus\nHow often do we feel bad about ourselves because we procrastinated, squandered our time, or did not accomplish something meaningful during the day? Making the right decisions takes practice. In this book, I invite you to practice the skills you have learned in BUAD 231 and the skills of focus, dedication, and consistency. Choose a day in the week and start by dedicating some fixed time to these problems (e.g., 15-30 minutes). The idea is to work on consistency (i.e., returning to the book weekly for a given amount of time). Some of us will find that concentrating is challenging. Your next task is to reduce distractions (i.e., the phone, t.v. or even your thoughts about the future). If you keep trying and returning to the book, you will improve at Business Statistics and learn to study with focus and consistency. All it takes is practice. Remember, you are what you practice!\nThe problems in this book are designed to help you master statistics and its application in R. I recommend reviewing Grolemund (2014) if you need additional help learning R. Finally, I have provided a list of concepts at the beginning of every chapter. Enjoy!"
  },
  {
    "objectID": "index.html#why-r",
    "href": "index.html#why-r",
    "title": "Business Statistics",
    "section": "Why R?",
    "text": "Why R?\nWe will be using R to apply the lessons we learn in BUAD 231. R is a language and environment for statistical computing and graphics. There are several advantages to using the R software for statistical analysis and data science. Some of the main benefits include:\n\nR is a powerful and flexible programming language that allows users to manipulate and analyze data in many different ways.\nR has a large and active community of users, who have developed a wide range of packages and tools for data analysis and visualization.\nR is free and open-source, which makes it accessible to anyone who wants to use it.\nR is widely used in academia and industry, which means that there are many resources and tutorials available to help users learn how to use it.\nR is well-suited for working with large and complex datasets, and it can handle data from many different sources.\nR can be easily integrated with other tools and software, such as databases, visualization tools, and machine learning algorithms.\n\nOverall, R is a powerful and versatile tool for data analysis and data science, and it offers many benefits to users who want to work with data."
  },
  {
    "objectID": "index.html#installing-r",
    "href": "index.html#installing-r",
    "title": "Business Statistics",
    "section": "Installing R",
    "text": "Installing R\nTo install R, visit the R webpage at https://www.r-project.org/. Once in the website, click on the CRAN hyperlink.\n\n\n\n\n\n\n\n\n\nHere you can select the CRAN mirror. Scroll down until you see USA. You are free to choose any mirror you like, I recommend using the Duke University mirror.\n\n\n\n\n\n\n\n\n\nOnce you click on the hyperlink, you will be prompted to choose the download for your operating system. Depending on your operating system, choose either a Windows or Macintosh download.\n\n\n\n\n\n\n\n\n\nFollow all prompts and complete installation."
  },
  {
    "objectID": "index.html#installing-rstudio",
    "href": "index.html#installing-rstudio",
    "title": "Business Statistics",
    "section": "Installing RStudio",
    "text": "Installing RStudio\nVisit the Posit website at https://posit.co. Once on the website, hover to the top right of the screen. You will see a “Download RStudio” blue button.\n\n\n\n\n\n\n\n\n\nNext, scroll down until you reach the RStudio desktop section. Click once more on “Download RStudio”. You can now just jump to Step 2 since you have already downloaded R. Finally, choose the desired download depending on your operating system.\n\n\n\n\n\n\n\n\n\nIt is important to note that RStudio will not work if R is not installed. You can think of R as the engine and RStudio as the interface."
  },
  {
    "objectID": "index.html#posit-cloud",
    "href": "index.html#posit-cloud",
    "title": "Business Statistics",
    "section": "Posit Cloud",
    "text": "Posit Cloud\nIf you do not wish to install R, you can always use the cloud version. To do this, visit https://posit.cloud/. On the main page click on the “Sign Up” button.\n\n\n\n\n\n\n\n\n\nChoose the “Cloud Free” option and log in using your Google credentials (if you have a Google account) or sign up if you want to create a new account.\n\n\n\n\nGrolemund, Garret. 2014. “Hands-on Programming with r.” https://jjallaire.github.io/hopr/."
  },
  {
    "objectID": "02-DescriptiveI.html#concepts",
    "href": "02-DescriptiveI.html#concepts",
    "title": "1  Descriptive Stats I",
    "section": "1.1 Concepts",
    "text": "1.1 Concepts\n\nData and Types of Data\nData are facts and figures collected, analyzed and summarized for presentation and interpretation. Data can be classified as:\n\nCross Sectional Data refers to data collected at the same (or approximately the same) point in time. Ex: NFL standings in 1980 or Country GDP in 2015.\nTime Series Data refers to data collected over several time periods. Ex: U.S. inflation rate from 2000-2010 or Tesla deliveries from 2016-2022.\nStructured Data resides in a predefined row-column format (tidy).\nUnstructured Data do not conform to a pre-defined row-column format. Ex: Text, video, and other multimedia.\n\n\n\nData Sets, Variables and Scales of Measurement\nA data set contains all data collected for a particular study. Data sets are composed of:\n\nElements are the entities on which data are collected. Ex: Football teams, countries, and individuals.\nObservations are the set of measurements obtained for a particular element.\nVariables are a set of characteristics collected for each element.\n\nThe scales of measurements determine the amount and type of information contained in each variable. In general, variables can be classified as categorical or numerical.\n\nCategorical (qualitative) data includes labels or names to identify an attribute of each element. Categorical data can be nominal or ordinal.\n\nWith nominal data, the order of the categories is arbitrary. Ex: Marital Status, Race/Ethnicity, or NFL division.\nWith ordinal data, the order or rank of the categories is meaningful. Ex: Rating, Difficulty Level, or Spice Level.\n\nNumerical (quantitative) include numerical values that indicate how many (discrete) or how much (continuous). The data can be either interval or ratio.\n\nWith interval data, the distance between values is expressed in terms of a fixed unit of measure. The zero value is arbitrary and does not represent the absence of the characteristic. Ratios are not meaningful. Ex: Temperature or Dates.\nWith ratio data, the ratio between values is meaningful. The zero value is not arbitrary and represents the absence of the characteristic. Ex: Prices, Profits, Wins.\n\n\n\n\nUseful R Functions\nBase R has some important functions that are helpful when dealing with data. Below is a list that might come handy.\n\nThe na.omit() function removes any observations that have a missing value (NA). The resulting data frame has only complete cases.\nThe nrow() and ncol() functions return the number of rows and columns respectively from a data frame.\nThe is.na() function returns a vector of True and False that specify if an entry is missing (NA) or not.\nThe summary() function returns a collection of descriptive statistics from a data frame (or vector). The function also returns whether there are any missing values (NA) in a variable.\nThe as.integer(), as.factor(), as.double(), are functions used to coerce your data into a different scale of measurement.\n\nThe dplyr package has a collection of functions that are useful for data manipulation and transformation. If you are interested in this package you can refer to Wickham (2017). To install, run the following command in the console install.packages(\"dplyr\").\n\nThe arrange() function allows you to sort data frames in ascending order. Pair with the desc() function to sort the data in descending order.\nThe filter() function allows you to subset the rows of your data based on a condition.\nThe select() function allows you to select a subset of variables from your data frame."
  },
  {
    "objectID": "02-DescriptiveI.html#exercises",
    "href": "02-DescriptiveI.html#exercises",
    "title": "1  Descriptive Stats I",
    "section": "1.2 Exercises",
    "text": "1.2 Exercises\nThe following exercises will help you test your knowledge on the Scales of Measurement. They will also allow you to practice some basic data “wrangling” in R. In these exercises you will:\n\nIdentify numerical and categorical data.\nClassify data according to their scale of measurement.\nSort and filter data in R.\nHandle missing values (NA’s) in R.\n\nAnswers are provided below. Try not to peak until you have a formulated your own answer and double checked your work for any mistakes.\n\nExercise 1\nA bookstore has compiled data set on their current inventory. A portion of the data is shown below:\n\n\n\nTitle\nPrice\nYear Published\nRating\n\n\n\n\nFrankenstein\n5.49\n1818\n4.2\n\n\nDracula\n7.60\n1897\n4.0\n\n\n…\n…\n…\n…\n\n\nSleepy Hollow\n6.95\n1820\n3.8\n\n\n\n\nWhich of the above variables are categorical and which are numerical?\nWhat is the measurement scale of each of the above variable?\n\n\n\nExercise 2\nA car company tracks the number of deliveries every quarter. A portion of the data is shown below:\n\n\n\nYear\nQuarter\nDeliveries\n\n\n\n\n2016\n1\n14800\n\n\n2016\n2\n14400\n\n\n…\n…\n…\n\n\n2022\n3\n343840\n\n\n\n\nWhat is the measurement scale of the Year variable? What are the strengths and weaknesses of this type of measurement scale?\nWhat is the measurement scale for the Quarter variable? What is the weakness of this type of measurement scale?\nWhat is the measurement scale for the Deliveries variable? What are the strengths of this type of measurement scale?\n\n\n\nExercise 3\nUse the airquality data set included in R for this problem.\n\nSort the data by Temp, Ozone, and Wind all in descending order. What is the day and month of the first observation on the sorted data?\nSort the data only by Temp in descending order. Of the \\(10\\) hottest days, how many of them were in July?\nHow many missing values are there in the data set? What rows have missing values for Solar.R?\nRemove all observations that have a missing values. Create a new object called CompleteAG.\nWhen using CompleteAG, how many days was the temperature at least \\(60\\) degrees?\nWhen using CompleteAG, how many days was the temperature within [\\(55\\),\\(75\\)] degrees and an Ozone below \\(20\\)?"
  },
  {
    "objectID": "02-DescriptiveI.html#answers",
    "href": "02-DescriptiveI.html#answers",
    "title": "1  Descriptive Stats I",
    "section": "1.3 Answers",
    "text": "1.3 Answers\n\nExercise 1\n\nThe variables Title and Rating are categorical whereas Price and Year are numerical.\nThe measurement scale is nominal for Title, ordinal for Ratio, ratio for Price, and interval for Year. Recall, that the nominal and ratio scales represent the least and most sophisticated levels of measurement, respectively.\n\n\n\nExercise 2\n\nThe variable Year is measured on the interval scale because the observations can be ranked, categorized and measured when using this kind of scale. However, there is no true zero point so we cannot calculate meaningful ratios between years.\nThe variable Quarter is measured on the nominal scale, even though it contains numbers. It is the least sophisticated level of measurement because if we are presented with nominal data, all we can do is categorize or group the data.\nThe variable Deliveries is measured on the ratio scale. It is the strongest level of measurement because it allows us to categorize and rank the data as well as find meaningful differences between observations. Also, with a true zero point, we can interpret the ratios between observations.\n\n\n\nExercise 3\n\nThe day and month of the first observation is August 28th.\n\nThe easiest way to sort in R is by using the dplyr package. Specifically, the arrange() function within the package. Let’s also use the desc() function to make sure that the data is sorted in descending order. We can use indexing to retrieve the first row of the sorted data set.\n\nlibrary(dplyr)\nSortedAQ<-arrange(airquality,desc(Temp),desc(Ozone),desc(Wind))\nSortedAQ[1,]\n\n  Ozone Solar.R Wind Temp Month Day\n1    76     203  9.7   97     8  28\n\n\n\nOf the \\(10\\) hottest days only two were in July.\n\nWe can use the arrange() function one more time for this question. Then we can use indexing to retrieve the top \\(10\\) observations.\n\nSortedAQ2<-arrange(airquality,desc(Temp))\nSortedAQ2[1:10,]\n\n   Ozone Solar.R Wind Temp Month Day\n1     76     203  9.7   97     8  28\n2     84     237  6.3   96     8  30\n3    118     225  2.3   94     8  29\n4     85     188  6.3   94     8  31\n5     NA     259 10.9   93     6  11\n6     73     183  2.8   93     9   3\n7     91     189  4.6   93     9   4\n8     NA     250  9.2   92     6  12\n9     97     267  6.3   92     7   8\n10    97     272  5.7   92     7   9\n\n\n\nThere are a total of \\(44\\) missing values. Ozone has \\(37\\) and Solar.R has \\(7\\). Rows \\(5\\), \\(6\\), \\(11\\), \\(27\\), \\(96\\), \\(97\\), \\(98\\) are missing for Solar.R.\n\nWe can easily identify missing values with the summary() function.\n\nsummary(airquality)\n\n     Ozone           Solar.R           Wind             Temp      \n Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :56.00  \n 1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:72.00  \n Median : 31.50   Median :205.0   Median : 9.700   Median :79.00  \n Mean   : 42.13   Mean   :185.9   Mean   : 9.958   Mean   :77.88  \n 3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00  \n Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00  \n NA's   :37       NA's   :7                                       \n     Month            Day      \n Min.   :5.000   Min.   : 1.0  \n 1st Qu.:6.000   1st Qu.: 8.0  \n Median :7.000   Median :16.0  \n Mean   :6.993   Mean   :15.8  \n 3rd Qu.:8.000   3rd Qu.:23.0  \n Max.   :9.000   Max.   :31.0  \n                               \n\n\nTo view the rows that have NA’s in them, we can use the is.na() function and indexing. Below we see that \\(7\\) values are missing for the Solar.R variable in the months \\(5\\) and \\(8\\) combined.\n\nairquality[is.na(airquality$Solar.R),]\n\n   Ozone Solar.R Wind Temp Month Day\n5     NA      NA 14.3   56     5   5\n6     28      NA 14.9   66     5   6\n11     7      NA  6.9   74     5  11\n27    NA      NA  8.0   57     5  27\n96    78      NA  6.9   86     8   4\n97    35      NA  7.4   85     8   5\n98    66      NA  4.6   87     8   6\n\n\n\nTo create the new object of complete observations we can use the na.omit() function.\n\n\nCompleteAQ<-na.omit(airquality)\n\n\nThere were \\(107\\) days where the temperature was at least \\(60\\).\n\nUsing base R we have:\n\nnrow(CompleteAQ[CompleteAQ$Temp>=60,])\n\n[1] 107\n\n\nWe can also use dplyr for this question. Specifically, using the filter() and nrow() functions we get:\n\nnrow(filter(CompleteAQ,Temp>=60))\n\n[1] 107\n\n\n\nThere were \\(24\\) days where the temperature was between \\(55\\) and \\(75\\) and the ozone level was below \\(20\\).\n\nUsing base R we have:\n\nnrow(CompleteAQ[CompleteAQ$Temp>55 & CompleteAQ$Temp<75 & CompleteAQ$Ozone<20,])\n\n[1] 24\n\n\nUsing the filter() function once more we get:\n\nnrow(filter(CompleteAQ,Temp>55,Temp<75,Ozone<20))\n\n[1] 24\n\n\n\n\n\n\nWickham, Hadley. 2017. “R for Data Science.” https://r4ds.hadley.nz."
  },
  {
    "objectID": "03-DescriptiveII.html#concepts",
    "href": "03-DescriptiveII.html#concepts",
    "title": "2  Descriptive Stats II",
    "section": "2.1 Concepts",
    "text": "2.1 Concepts\n\nFrequency\nA frequency distribution is a tabular summary of data showing the number of items in each of several non-overlapping classes.\n\nThe relative frequency is calculated by \\(f_{i}/n\\), where \\(f_{i}\\) is the frequency of class \\(i\\) and \\(n\\) is the total frequency.\nThe cumulative frequency shows the number of data items with values less than or equal to the upper class limit of each class.\nThe cumulative relative frequency is given by \\(cf_{i}/n\\), where \\(cf_{i}\\) is the cumulative frequency of class \\(i\\).\n\n\n\nPlots\nA bar plot illustrates the frequency distribution of qualitative data.\n\nIs an illustration for qualitative data.\nIncludes the classes in the horizontal axis and frequencies or relative frequencies in the vertical axis.\nHas gaps between each bar.\n\nA histogram illustrates the frequency distribution of quantitative data.\n\nIs an illustration for quantitative data.\nThere are no gaps between the bars.\nThe number, width and limits of each class must be determined.\n\nThe number of classes can be determined by the \\(2^k\\) rule: select \\(k\\) such that \\(2^k\\) is greater than the number of observations by the smallest amount.\nThe width of the class is approximately range/(# of Classes). The value should be rounded up.\nThe limits should be chosen so that each point belongs to only one class.\n\n\n\n\nUseful R Functions\nThe table() command generates frequency distributions or contingency tables if two variables are used.\nThe prop.table() command generates relative frequency distributions from an object that contains a table.\nThe cut() function generates class limits and bins used in frequency distributions (and histograms) for quantitative data.\nBase R has the barplot() function for categorical variable, histogram() function for numerical data, and the plot() function for line charts or scatter plots. Below are some arguments that are helpful when plotting.\n\nmain: used to set the plot’s title. The title should be entered as a character.\ncol: used to set the color of the plot. Hex and RGB values are allowed as inputs. The color should be entered as a character.\nxlab and ylab: are used to set the labels for the \\(x\\) and \\(y\\) axis respectively. The labels should be entered as characters.\nlegend() is a function to customize the legend of a graph. This argument may be used with the plot(), barplot() or histogram() functions.\n\nx: used to set the location of the legend in the plotting area. Ex: “bottomleft”.\nlegend: a vector specifying the legend names to be included.\ncol: a vector specifying the color of each item in the legend."
  },
  {
    "objectID": "03-DescriptiveII.html#exercises",
    "href": "03-DescriptiveII.html#exercises",
    "title": "2  Descriptive Stats II",
    "section": "2.2 Exercises",
    "text": "2.2 Exercises\nThe following exercises will help you practice summarizing data with tables and simple graphs. In particular, the exercises work on:\n\nDeveloping frequency distributions for both categorical and numerical data.\nConstructing bar charts, histograms, and line charts.\nCreating contingency tables.\n\nAnswers are provided below. Try not to peak until you have a formulated your own answer and double checked your work for any mistakes.\n\nExercise 1\nInstall the ISLR2 package in R. You will need the BrainCancer data set to answer this question.\n\nConstruct a frequency and relative frequency table of the Diagnosis variable. What was the most common diagnosis? What percentage of the sample had this diagnosis?\nConstruct a bar chart. Summarize the findings.\nConstruct a contingency table that shows the Diagnosis along with the Status. Which diagnosis had the highest number of non-survivals (0)? What was the survival rate of this diagnosis?\nConstruct a stacked column chart. Which two Diagnosis and Status combinations are the most frequent?\n\n\n\nExercise 2\nYou will need the airquality data set (in base R) to answer this question.\n\nConstruct a frequency distribution for Temp. Use five intervals with widths of \\(50<x\\le60\\); \\(60<x\\le70\\); etc. Which interval had the highest frequency? How many times was the temperature between \\(50\\) and \\(60\\) degrees?\nConstruct a relative frequency, cumulative frequency and the relative cumulative frequency distributions. What proportion of the time was Temp between \\(50\\) and \\(60\\) degrees? How many times was the Temp \\(70\\) degrees or less? What proportion of the time was Temp more than \\(70\\) degrees?\nConstruct the histogram. Is the distribution symmetric? If not, is it skewed to the left or right?\n\n\n\nExercise 3\nYou will need the Portfolio data set from the ISLR2 package to answer this question.\n\nConstruct a line chart that shows the returns over time for each portfolio (X and Y) by using two lines each with a unique color. Assume the data is for the period \\(1901\\) to \\(2000\\). Include also a legend that matches colors to portfolios."
  },
  {
    "objectID": "03-DescriptiveII.html#answers",
    "href": "03-DescriptiveII.html#answers",
    "title": "2  Descriptive Stats II",
    "section": "2.3 Answers",
    "text": "2.3 Answers\n\nExercise 1\n\nThe most common diagnosis is Meningioma, a slow-growing tumor that forms from the membranous layers surrounding the brain and spinal cord. The diagnosis represents about \\(48.28\\)% of the sample.\n\nStart by loading the ISLR2 package. To construct the frequency distribution table, use the table() function.\n\nlibrary(ISLR2)\ntable(BrainCancer$diagnosis)\n\n\nMeningioma  LG glioma  HG glioma      Other \n        42          9         22         14 \n\n\nThe relative frequency distribution can be easily retrieved by saving the frequency table in an object and then using the prop.table() function.\n\nfreq<-table(BrainCancer$diagnosis)\nprop.table(freq)\n\n\nMeningioma  LG glioma  HG glioma      Other \n 0.4827586  0.1034483  0.2528736  0.1609195 \n\n\n\nThe majority of diagnosis are Meningioma. Low grade glioma is the least common of diagnosis. High grade glioma and other diagnosis have about the same frequency.\n\nTo construct the bar chart use the barplot() function in R.\n\nbarplot(freq, col = \"#F5F5F5\", ylim=c(0,50))\n\n\n\n\n\n\\(33\\) people did not survive Meningioma. The survival rate of Meningioma is only \\(21.43\\)%.\n\nUse the table() function one more time to create the contingency table for the two variables.\n\n(freq2<-table(BrainCancer$status,BrainCancer$diagnosis))\n\n   \n    Meningioma LG glioma HG glioma Other\n  0         33         5         5     9\n  1          9         4        17     5\n\n\nTo get the survival rates, we can use the prop.table() function once again.\n\nprop.table(freq2,margin = 2)\n\n   \n    Meningioma LG glioma HG glioma     Other\n  0  0.7857143 0.5555556 0.2272727 0.6428571\n  1  0.2142857 0.4444444 0.7727273 0.3571429\n\n\n\nMeningioma and not surviving is the most common with \\(33\\) occurrences. High grade glioma and surviving is the the second most common.\n\nUse the barplot() function one more time to construct the stacked column chart.\n\nbarplot(table(BrainCancer$status,BrainCancer$diagnosis),\n        legend.text = c(\"Not Survived\",\"Survived\"), ylim=c(0,50))\n\n\n\n\n\n\nExercise 2\n\nThe highest frequency is in the \\(80 < x ≤ 90\\) bin. \\(8\\) temperatures were between \\(50 < x ≤ 60\\) degrees.\n\nCreate a vector containing the intervals desired by using the seq() function.\n\nintervals <- seq(50, 100, by=10)\n\nNext use the cut() function to create the cuts for the histogram.\n\nintervals.cut <- cut(airquality$Temp, intervals, left=FALSE, right=TRUE)\n\nThe frequency distribution can be obtained by using the table() function on the interval.cut object created above.\n\ntable(intervals.cut)\n\nintervals.cut\n (50,60]  (60,70]  (70,80]  (80,90] (90,100] \n       8       25       52       54       14 \n\n\n\nThe temperature was \\(5.22\\)% of the time between \\(50\\) and \\(60\\); The temperature was \\(70\\) or less \\(33\\) times; The temperature was above \\(70\\), \\(78.43\\)% of the time.\n\nTo get the relative frequency table, start by saving the proportion table into an object.Then you can use the prop.table() function.\n\nfreq<-table(intervals.cut) \nprop.table(freq)\n\nintervals.cut\n   (50,60]    (60,70]    (70,80]    (80,90]   (90,100] \n0.05228758 0.16339869 0.33986928 0.35294118 0.09150327 \n\n\nFor the cumulative distribution you can use the cumsum() function on the frequency distribution.\n\ncumulfreq<-cumsum(freq)\ncumulfreq\n\n (50,60]  (60,70]  (70,80]  (80,90] (90,100] \n       8       33       85      139      153 \n\n\nLastly, for the relative cumulative distribution table, you can use the cumsum() function on the relative frequency table.\n\ncumsum(prop.table(freq))\n\n   (50,60]    (60,70]    (70,80]    (80,90]   (90,100] \n0.05228758 0.21568627 0.55555556 0.90849673 1.00000000 \n\n\n\nThe distribution is not perfectly symmetric. It is skewed slightly to the left (see histogram.)\n\nUse the hist() function to create the histogram.\n\nhist(airquality$Temp, breaks=intervals, \n     right=TRUE,col=\"#F5F5F5\", main=\"Temperature in NY\", xlab=\"\")\n\n\n\n\n\n\nExercise 3\n\nFrom \\(1901\\) through \\(2000\\), both portfolios have behaved very similarly. Returns are between \\(-3\\)% and \\(3\\)%, there is no trend, and positive (negative) returns for X seem to match with positive (negative) returns of Y.\n\nYou can use the plot() function to create a plot of Portfolio Y. The line for Portfolio X can be added with the lines() function.\n\nplot(Portfolio$Y, \n     x=seq(1901,2000), type=\"l\", \n     col=\"black\", xlab=\"\", ylab=\"% Return\", ylim=c(-3,3), \n     xlim=c(1901,2000), lwd=2, axes = F)\naxis(side=1, labels=TRUE, font=1,las=1)\naxis(side=2, labels=TRUE, font=1,las=1)\nlines(Portfolio$X, x=seq(1901,2000), type=\"l\", \n      col=\"darkgrey\", lwd=2)\nlegend(x = \"bottomleft\",          \n       legend = c(\"Port Y\", \"Port X\"),  \n       lty = c(1, 1),           \n       col = c(\"black\", \"darkgrey\"),         \n       lwd = 2,\n       bty=\"n\")"
  },
  {
    "objectID": "04-DescriptiveIII.html#concepts",
    "href": "04-DescriptiveIII.html#concepts",
    "title": "3  Descriptive Statistics III",
    "section": "3.1 Concepts",
    "text": "3.1 Concepts\n\nMeasures of Central Location\nMeasures of Central Location determine where the center of a distribution lies.\n\nThe mean is the average value for a numerical variable. The sample statistic is estimated by \\(\\bar{x}=\\sum x_{i}/n\\), where \\(x_i\\) is observation \\(i\\), and \\(n\\) is the number of observations. The population parameter is defined as \\(\\mu=\\sum x_{i}/N\\).\nThe median is the value in the middle when data is organized in ascending order. When \\(n\\) is even, the median is the average between the two middle values.\nThe mode is the value with highest frequency from a set of observations.\nThe weighted mean uses weights to determine the importance of each data point of a variable. It is calculated by \\(\\frac{\\sum w_{i}x_{i}}{\\sum w_{i}}\\), where \\(w_{i}\\) are the weights associated to the values \\(x_{i}\\).\nThe geometric mean is a multiplicative average that is less sensitive to outliers. It is used to average growth rates or rated of return. It is calculated by \\(\\sqrt[n]{(1+r_1)*(1+r_2)...(1+r_n)}-1\\), where \\(\\sqrt[n]{}\\) is the \\(n_{th}\\) root, and \\(r_i\\) are the returns or growth rates.\n\n\n\nUseful R functions\nBase R has a collection of functions that calculate measures of central location.\n\nThe mean() function calculates the average of a vector of values.\nThe median() function returns the median of a vector of values.\nThe table() function provides us with a frequency distribution. We can then identify the mode(s) of the vector provided.\nThe summary() function returns a collection of descriptive statistics for a vector or data frame."
  },
  {
    "objectID": "04-DescriptiveIII.html#exercises",
    "href": "04-DescriptiveIII.html#exercises",
    "title": "3  Descriptive Statistics III",
    "section": "3.2 Exercises",
    "text": "3.2 Exercises\nThe following exercises will help you practice the measures of central location. In particular, the exercises work on:\n\nCalculating the mean, median, and the mode.\nCalculating the weighted average.\nApplying the geometric mean for growth rates and returns.\n\nAnswers are provided below. Try not to peak until you have a formulated your own answer and double checked your work for any mistakes.\n\nExercise 1\nFor the following exercises, make your calculations by hand and verify results using R functions when possible.\n\nUse the following observations to calculate the mean, the median, and the mode.\n\n\n\n8\n10\n9\n12\n12\n\n\n\nUse following observations to calculate the mean, the median, and the mode.\n\n\n\n-4\n0\n-6\n1\n-3\n-4\n\n\n\nUse the following observations, calculate the mean, the median, and the mode.\n\n\n\n20\n15\n25\n20\n10\n15\n25\n20\n15\n\n\n\n\n\n\nExercise 2\nDownload the ISLR2 package. You will need the OJ data set to answer this question.\n\nFind the mean price for Country Hill (PriceCH) and Minute Maid (PriceMM).\nFind the mean price of Country Hill (PriceCH) in store 1 and store 2 (StoreID). Which store had the better price?\nFind the mean price paid by Country Hill (PriceCH) purchasers (Purchase) in store 1 (StoreID)? How about store 2? Which store had the better price?\n\n\n\nExercise 3\n\nOver the past year an investor bought TSLA. She made these purchases on three occasions at the prices shown in the table below. Calculate the average price per share.\n\n\n\n\nDate\nPrice Per Share\nNumber of Shares\n\n\n\n\nFebruary\n250.34\n80\n\n\nApril\n234.59\n120\n\n\nAug\n270.45\n50\n\n\n\n\nWhat would have been the average price per share if the investor would have bought equal amounts of shares each month?\n\n\n\nExercise 4\n\nConsider the following observations for the consumer price index (CPI). Calculate the inflation rate (Growth Rate of the CPI) for each period.\n\n\n\n1.0\n1.3\n1.6\n1.8\n2.1\n\n\n\nSuppose that you want to invest $1000 dollars in a stock that is predicted to yield the following returns in the next four years. Calculate both the arithmetic mean and the geometric mean. Use the geometric mean to estimate how much money you would have by the end of year 4.\n\n\n\nYear\nAnnual Return\n\n\n\n\n1\n17.3\n\n\n2\n19.6\n\n\n3\n6.8\n\n\n4\n8.2"
  },
  {
    "objectID": "04-DescriptiveIII.html#answers",
    "href": "04-DescriptiveIII.html#answers",
    "title": "3  Descriptive Statistics III",
    "section": "3.3 Answers",
    "text": "3.3 Answers\n\nExercise 1\n\nTo find the mean we will use the following formula \\(( \\frac{1}{n} \\sum_{i=i}^{n} x_{i})\\). The summation of the values is \\(51\\) and the number of observations is \\(5\\). The mean is \\(51/5=10.2\\).\nThe median is found by locating the middle value when data is sorted in ascending order. The median in this example is \\(10\\).\nThe mode is the value with the highest frequency. In this example the mode is \\(12\\) since it is repeated twice and all other numbers appear only once.\n\nThe mean can be easily verified in R by using the mean() function:\n\nmean(c(8,10,9,12,12))\n\n[1] 10.2\n\n\nSimilarly, the median is easily verified by using the median() function:\n\nmedian(c(8,10,9,12,12))\n\n[1] 10\n\n\nWe can use the table() function to calculate frequencies and easily identify the mode.\n\ntable(c(8,10,9,12,12))\n\n\n 8  9 10 12 \n 1  1  1  2 \n\n\n\nThe mean is \\(-2.67\\), the median is \\(-3.5\\), the mode is \\(-4\\).\n\nThese mean is verified in R:\n\nmean(c(-4,0,-6,1,-3,-4))\n\n[1] -2.666667\n\n\nThe median in R:\n\nmedian(c(-4,0,-6,1,-3,-4))\n\n[1] -3.5\n\n\nFinally, the mode in R:\n\ntable(c(-4,0,-6,1,-3,-4))\n\n\n-6 -4 -3  0  1 \n 1  2  1  1  1 \n\n\n\nThe mean is \\(18.33\\), the median is \\(20\\), the data is bimodal with both \\(15\\) and \\(20\\) being modes.\n\nThese mean is verified in R:\n\nmean(c(20,15,25,20,10,15,25,20,15))\n\n[1] 18.33333\n\n\nThe median in R:\n\nmedian(c(20,15,25,20,10,15,25,20,15))\n\n[1] 20\n\n\nThe frequency distribution identifies the modes:\n\ntable(c(20,15,25,20,10,15,25,20,15))\n\n\n10 15 20 25 \n 1  3  3  2 \n\n\n\n\nExercise 2\n\nThe mean price for Country Hill is \\(1.87\\). The mean price for Minute Maid is \\(2.09\\).\n\nThe means can be easily found with the mean() function:\n\nlibrary(ISLR2)\nmean(OJ$PriceCH)\n\n[1] 1.867421\n\nmean(OJ$PriceMM)\n\n[1] 2.085411\n\n\n\nThe mean price at store 1 for Country Hill is \\(1.80\\) vs. \\(1.84\\) for store 2. The juice is cheaper at store 1.\n\nThe means for each store can be found by using indexing and a logical statement. The Country Hill mean price at store 1 is given by:\n\nmean(OJ$PriceCH[OJ$StoreID==1])\n\n[1] 1.803758\n\n\nThe Country Hill mean price at store 2 is given by:\n\nmean(OJ$PriceCH[OJ$StoreID==2])\n\n[1] 1.841216\n\n\n\nPurchasers of Country Hill at store 1 paid and average of \\(1.80\\) for Country Hill juice. At store 2 they paid \\(1.86\\). Once again the average price was lower at store 1.\n\nThe mean for Country Hill purchasers at store 1 is given by:\n\nmean(OJ$PriceCH[OJ$StoreID==1 & OJ$Purchase==\"CH\"])\n\n[1] 1.797176\n\n\nThe mean for Country Hill purchasers at store 2 is:\n\nmean(OJ$PriceCH[OJ$StoreID==2 & OJ$Purchase==\"CH\"])\n\n[1] 1.857383\n\n\n\n\nExercise 3\n\nThe average price of sale is found by using the weighted average formula. \\(\\frac{\\sum w_{i}x_{i}}{\\sum w_{i}}\\) The weights (\\(w_{i}\\)) are given by the number of shares bought and the values (\\(x_{i}\\)) are the prices. The weighted average is \\(246.802\\).\n\nIn R you can create two vectors. One holds the share price and the other one the number of shares bought.\n\nPricePerShare<-c(250.34,234.59,270.45)\nNumberOfShares<-c(80,120,50)\n\nNext, you can multiply the PricePerShare and NumberOfShares vectors to find the numerator and then use sum() function to find the denominator. The weighted average is:\n\n(WeightedAverage<-\n  sum(PricePerShare*NumberOfShares)/sum(NumberOfShares))\n\n[1] 246.802\n\n\n\nThe average if equal shares were bought would be \\(251.7933\\).\n\nIn R you can use the mean() function on the PricePerShare vector.\n\n(Average<-mean(PricePerShare))\n\n[1] 251.7933\n\n\n\n\nExercise 4\n\nThe inflation rate for each period is shown in the table below:\n\n\n\n\n30%\n23.08%\n12.5%\n16.67%\n\n\n\nIn R create an object to store the values of the CPI:\n\nCPI<-c(1,1.3,1.6,1.8,2.1)\n\nNext use the diff() function to find the difference between the end value and start value. Divide the result by a vector of starting value and multiply times 100.\n\n(Inflation<-100*diff(CPI)/CPI[1:4])\n\n[1] 30.00000 23.07692 12.50000 16.66667\n\n\n\nAt the end of 4 years it is predicted that you would have \\(1621.17\\) dollars. Each year you would have gained \\(12.84\\)% on average.\n\nIn R include the annual rates in a vector:\n\ngrowth<-c(0.173,0.196,0.068,0.082)\n\nThe arithmetic mean is:\n\n100*mean(growth)\n\n[1] 12.975\n\n\nThe geometric mean is:\n\n(geom<-((prod(1+growth))^(1/4)-1)*100)\n\n[1] 12.8384\n\n\nAt the end of the four years we would have:\n\n1000*(1+geom/100)^4\n\n[1] 1621.167"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Grolemund, Garret. 2014. “Hands-on Programming with r.” https://jjallaire.github.io/hopr/.\n\n\nWickham, Hadley. 2017. “R for Data Science.” https://r4ds.hadley.nz."
  }
]